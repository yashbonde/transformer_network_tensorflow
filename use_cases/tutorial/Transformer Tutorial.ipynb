{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need\n",
    "\n",
    "This is the title of this notebook, here I train transformer network from 'Attention is all you need' on a toy training set. You can read the [paper](https://arxiv.org/pdf/1706.03762.pdf) and understand more about it.\n",
    "\n",
    "### History\n",
    "\n",
    "Most common method used to implement machine translation/language model is to use RNN, a simple LSTM Encoder module that generates a final hidden state and LSTM Decoder that uses that hidden state to generate output. The main issue with these models has always not only been the trianing time it took but also the travel each word had to travel in sequence.\n",
    "\n",
    "Till date the most efficient models have used atttention mechanisms. At the gist attention mechanism can tell by going back which word in input sequence it should focus on inorder to predict the word at output right now. The most common attention mechanism is self-attention. The main problem with using self-attention mechanism is the lack of temporal knowledge, since it goes over the entire input it has no way of knowing the order/position (temporal information) of word.\n",
    "\n",
    "Another models that have been succesfull at this task are Convolution Models. The main advantage being the processing speed and training time, since the GPUs are much more optimised to perform this kind of operations.\n",
    "\n",
    "##### You can learn more about the benefit of using this over RNN and it's crucial details on the [blog post]() I wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# importing the dependencies\n",
    "%matplotlib inline\n",
    "import os # os iteraction\n",
    "import numpy as np # linear algebra\n",
    "import tensorflow as tf # ML\n",
    "import matplotlib.pyplot as plt # visualisation\n",
    "from transformer import Transformer # our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the toy dataset\n",
    "\n",
    "Due to my hardware limitations, I can't train the model on actual dataset. But will train it on toy dataset, made up of fake language. The best way to image this is using visualisations, and this is what we have done. For simplycity we are keeping parrameters of both the languages same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "vocab_size = 127 # vocabulary size\n",
    "max_len_sent = 30 # size of the longest sentence\n",
    "min_len_sent = 6 # size of the smallest sentence\n",
    "num_samples = 8000 # number of sentences\n",
    "test_split = 0.05 # ratio of testing data to total data\n",
    "batch_size = 50 # mini-batch size\n",
    "\n",
    "# paddings - since this is a toy task, we won't need end or start init values\n",
    "# neither would we need these: <UNK>, <START>, <EOS>, etc.\n",
    "\n",
    "# language 1\n",
    "len_senteces = [np.random.randint(low = min_len_sent, high = max_len_sent+1) for _ in range(num_samples)]\n",
    "data_1 = np.asarray([np.random.randint(vocab_size, size = len_senteces[i]) for i in range(num_samples)])\n",
    "print(data_1.shape)\n",
    "\n",
    "# language 2\n",
    "data_2 = np.asarray([np.random.randint(vocab_size, size = len_senteces[i]) for i in range(num_samples)])\n",
    "print(data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the information about lengths\n",
    "lens_d1 = [len(i) for i in data_1]\n",
    "lens_d2 = [len(i) for i in data_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's visualise lenghts distribution\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.hist(x = lens_d1, bins='auto', color='#aa08aa', alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 47 25 36  6 23]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11927c5f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input sentence\n",
    "print(data_1[0])\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.axis('off')\n",
    "plt.title('Language 1')\n",
    "plt.imshow(np.reshape(data_1[0], [1,-1]), cmap = 'Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 76 118  62  19 106  51]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119239ef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# target sentence\n",
    "print(data_2[0])\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.axis('off')\n",
    "plt.title('Language 2')\n",
    "plt.imshow(np.reshape(data_2[0], [1,-1]), cmap = 'Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training input is: (7600, 1)\n",
      "Shape of test data: (400, 1)\n"
     ]
    }
   ],
   "source": [
    "# shuffle the indexes\n",
    "all_ids = np.arange(0, len(data_1))\n",
    "np.random.shuffle(all_ids)\n",
    "test_ids = all_ids[:int(test_split * len(data_1))]\n",
    "train_ids = all_ids[int(test_split * len(data_1)):]\n",
    "\n",
    "# get data according to these idexes\n",
    "train_input = data_1[train_ids]\n",
    "train_target = data_2[train_ids]\n",
    "test_input = data_1[test_ids]\n",
    "test_target = data_2[test_ids]\n",
    "\n",
    "# reshape the data as [<NUM_SAMPLES>, 1, <NUM_WORDS>]\n",
    "train_input = np.expand_dims(train_input, axis = 1)\n",
    "train_target = np.expand_dims(train_target, axis = 1)\n",
    "test_input = np.expand_dims(test_input, axis = 1)\n",
    "test_target = np.expand_dims(test_target, axis = 1)\n",
    "\n",
    "# print stats\n",
    "print('Shape of training input is:', train_input.shape)\n",
    "print('Shape of test data:', test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "e_dim = 32 # embedding dimension\n",
    "'''\n",
    "Embedding dimension should be less than the vocab size, this is due to common sense. The most\n",
    "ideal embedding mechanism would be one where the most of the words are orthogonal to each other.\n",
    "I say most because then similar words can be clustered together while the rest will be away from\n",
    "each other. So in fact using one-hot encoding for words is not an ideal embedding mechanism.\n",
    "Since out vocab size is quite limted our embedding dimension should also be correspondingly\n",
    "smaller. We will be using simple embedding matrix in numpy and get the values from those. In\n",
    "practice we should always put all these functions in the model and use the tf.nn.embedding_lookup.\n",
    "But this is a toy task and we can get away with it.\n",
    "'''\n",
    "print(e_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "We first initialise our network according to our requirements, i.e. we trim down on the model parameters and number of stacks used in encoder/decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(VOCAB_SIZE = vocab_size, DIM_MODEL = e_dim, NUM_HEADS = 4, FF_MID = 36)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ab3910a40a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# for each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AI/personal/transformer/transformer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input_seq, output_seq, is_training)\u001b[0m\n\u001b[1;32m    294\u001b[0m                                 \u001b[0;31m# for each value get the embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                                 \u001b[0membed_val_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                                 \u001b[0membed_val_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                                 \u001b[0mpos_enc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIM_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1307\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's train this baby\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "for i in range(num_epochs):\n",
    "    # for each epoch\n",
    "    loss_epoch = []\n",
    "    for i in range(len(train_input)):\n",
    "        # for each sample\n",
    "        gen, loss = model.run(input_seq = train_input[i], output_seq = train_target[i], is_training = True)\n",
    "        loss_epoch.append(np.mean(loss))\n",
    "        \n",
    "    print('average loss this epoch:', np.mean(loss_epoch[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations\n",
    "\n",
    "Due to my hardware limitations, training these models is very difficult. You are free to use these as you want"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
